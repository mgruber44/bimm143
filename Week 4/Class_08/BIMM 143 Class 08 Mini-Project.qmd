---
title: "Class 8 Mini-Project"
name: Max Gruber
date: 4/28/2023
format: gfm
---

# Preparing the data

```{r}
#assigning the .csv file to an object
fna.data <- "WisconsinCancer.csv"
#reading the csv and assigning it to a new object
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

### Setting up the data

```{r}
#removing the first column of the dataset
wisc.data <- wisc.df[,-1]

#making a new vector for the diagnosis column
diagnosis <- factor(wisc.df$diagnosis)
```

# Exploratory Data Analysis

### **Q1**. How many observations are in this dataset?

```{r}
#using dim() to see the rows and columns of the dataset
dim(wisc.df)
```

There are 569 observations in this dataset.

### **Q2**. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

There are 212 observations that have a malignant diagnosis.

### **Q3**. How many variables/features in the data are suffixed with `_mean`?

```{r}
length(grep('_mean', colnames(wisc.df)))
```

There are 10.

# 2. Principal Component Analysis

## Performing PCA

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)

wisc.pr <- prcomp(wisc.data, scale =TRUE)
summary(wisc.pr)
```

### Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

.4427 (44.27%) is the proportion of the original variance captured by the PC1.

### Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required.

### **Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required.

```{r}
biplot(wisc.pr)
```

### **Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?

This plot is incredibly cluttered and hard to read, making it difficult to understand as there is no general trend that is really visible to someone trying to pull information from the plot.

```{r}
plot( wisc.pr$x[,1:2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

### Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[, -2], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

#or

plot(wisc.pr$x[, 1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

As PC2 explains more variance in the original data than PC3, the first plot generated has a cleaner separation between the two subgroups. The plots overall show that PC1 is capturing a difference between malignant (red) and benign (black) samples.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

# Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

# Communicating PCA results

### **Q9.** For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`? This tells us how much this original feature contributes to the first PC.

```{r}
(wisc.pr$rotation[,1])
```

-0.26085376

# 3. Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

#Euclidean distnaces between pairs of observations
data.dist <- dist(data.scaled)

#creation of a hierarchical clustering model
wisc.hclust <- hclust(data.dist, method = "complete")
```

### Q10. Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

The height is 19.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h= 19)
table(wisc.hclust.clusters, diagnosis)
```

### **Q12.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning.

```{r}
wisc.hclust.single <- hclust(data.dist, method = "single")
plot(wisc.hclust.single)

wisc.pr.hclust <- hclust(data.dist, method = "ward.D2")
plot(wisc.pr.hclust)

```

The "ward.D2" method gives my favorite results, as the clustering is incredibly clear and easier to interpret compared to the other methods. It is also more visually appealing in my opinion.

# 4. Combining methods

```{r}
wisc.pr.hclust <- hclust(data.dist, method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)

table(grps,diagnosis)

plot(wisc.pr$x[,1:2], col=grps)

plot(wisc.pr$x[,1:2], col=diagnosis)


g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
plot(wisc.pr$x[,1:2], col=g)

## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

### Q13. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

The newly created model separates out the two diagnoses well, creating compacted clusters in four groups that more clearly define the diagnoses. The clustering yields a more digestible/easy to understand idea of how the data is spread.

### **Q14**. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the `table()` function to compare the output of each model (`wisc.km$cluster` and`wisc.hclust.clusters`) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Prior to PCA, the hierarchical clustering models feature more spread and are less compact. It is harder to interpret what the data is actually trying to show, and the difference in clustering between diagnoses is not as clear.

# 6. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc

plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

### **Q16.** Which of these new patients should we prioritize for follow up based on your results?

```{r}
g
```

In the generated chart, the black clustering indicates 'benign' and the red clustering indicates 'metastatic' tumors. The normal cells (benign) tend to cluster, whereas there is more spread in the metastatic cells.

As such, we should prioritize patient 2 because they are more likely to have a metastatic tumor, as patient 2 falls within the metastatic clustering whereas patient 1 falls within the benign clustering.
